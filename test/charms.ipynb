{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from TALENT.model.utils import get_deep_args,get_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd96340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_kv, dim_out, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = (nn.MultiheadAttention\n",
    "                     (embed_dim=dim_out, num_heads=num_heads, dropout=dropout, batch_first=True))\n",
    "        self.proj_q = nn.Linear(dim_q, dim_out)\n",
    "        self.proj_kv = nn.Linear(dim_kv, dim_out)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        q_proj = self.proj_q(q)\n",
    "        kv_proj = self.proj_kv(kv)\n",
    "        attn_output, _ = self.attn(q_proj.unsqueeze(1), kv_proj.unsqueeze(1), kv_proj.unsqueeze(1))\n",
    "        return attn_output.squeeze(1)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str = 'resnet', in_dims: int = 2048,\n",
    "                 out_dims: int = 129,\n",
    "                 n_num_features: int = 0,\n",
    "                 cat_cardinalities: list = [],\n",
    "                 d_token: int = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        if model_name == \"resnet\":\n",
    "            backbone = models.resnet50(weights=models.resnet.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            in_dims = backbone.fc.in_features\n",
    "            img_fc = nn.Sequential(\n",
    "                nn.Linear(in_dims, 1024),\n",
    "                nn.Linear(1024, out_dims)\n",
    "            )\n",
    "            backbone.fc = Identity()\n",
    "        elif model_name == \"densenet\":\n",
    "            backbone = models.densenet121(weights=models.densenet.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "            in_dims = backbone.classifier.in_features\n",
    "            img_fc = nn.Sequential(\n",
    "                nn.Linear(in_dims, 1024),\n",
    "                nn.Linear(1024, out_dims)\n",
    "            )\n",
    "            backbone.classifier = Identity()\n",
    "        elif model_name == \"inception\":\n",
    "            backbone = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "            in_dims = backbone.fc.in_features\n",
    "            img_fc = nn.Sequential(\n",
    "                nn.Linear(in_dims, 1024),\n",
    "                nn.Linear(1024, out_dims)\n",
    "            )\n",
    "            backbone.fc = Identity()\n",
    "        elif model_name == \"mobilenet\":\n",
    "            backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "            in_dims = backbone.classifier[-1].in_features\n",
    "            img_fc = nn.Sequential(\n",
    "                nn.Linear(in_dims, 1024),\n",
    "                nn.Linear(1024, out_dims)\n",
    "            )\n",
    "            backbone.classifier[-1] = Identity()\n",
    "\n",
    "        self.in_dims = in_dims\n",
    "        self.table_dim = n_num_features + len(cat_cardinalities)\n",
    "\n",
    "        self.con_fc_num = n_num_features\n",
    "        self.cat_fc_num = len(cat_cardinalities)\n",
    "        self.tab_model = self.get_ftt_from_talent()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.img_fc = img_fc\n",
    "        self.device = None\n",
    "\n",
    "        tab_embd_dim = d_token * n_num_features + len(cat_cardinalities) * d_token\n",
    "        cross_attn_out_dim = tab_embd_dim\n",
    "\n",
    "        self.img_to_tab_cross_attn = CrossAttention(dim_q=in_dims, dim_kv=tab_embd_dim, dim_out=cross_attn_out_dim)\n",
    "        self.tab_to_img_cross_attn = CrossAttention(dim_q=tab_embd_dim, dim_kv=in_dims, dim_out=cross_attn_out_dim)\n",
    "        self.img_to_tab_dim = nn.Linear(in_dims, tab_embd_dim)\n",
    "\n",
    "        self.cross_dim = self.table_dim * d_token\n",
    "        self.image_cross_proj = nn.Linear(in_dims, self.cross_dim)\n",
    "        # self.table_cross_proj = nn.Linear(self.table_dim, self.cross_dim)\n",
    "\n",
    "        self.concat_fc = nn.Sequential(\n",
    "            nn.Linear(tab_embd_dim * 3 + in_dims, out_dims),\n",
    "        )\n",
    "\n",
    "    def get_ftt_from_talent(self):\n",
    "        args, _, _ = get_deep_args()\n",
    "        args.model_type = \"ftt\"\n",
    "        method = get_method(args.model_type)(args, True)\n",
    "        return method.model\n",
    "    \n",
    "    def InfoMax_loss(self, x1, x2):\n",
    "        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n",
    "        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n",
    "        bs = x1.size(0)\n",
    "        s = torch.matmul(x1, x2.permute(1, 0))\n",
    "        mask_joint = torch.eye(bs).cuda()\n",
    "        mask_marginal = 1 - mask_joint\n",
    "\n",
    "        Ej = (s * mask_joint).mean()\n",
    "        Em = torch.exp(s * mask_marginal).mean()\n",
    "        # decoupled comtrastive learning?!!!!\n",
    "        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n",
    "        infomax_loss = - (Ej - torch.log(Em))  # / Em\n",
    "        return infomax_loss\n",
    "\n",
    "    def InfoMin_loss(self, cross_img_feats, cross_tab_feats):\n",
    "        \"\"\"\n",
    "        最小化 cross_img_feats 和 cross_tab_feats 的互信息。\n",
    "        使用 KL 散度方法：\n",
    "        I(cross_img_feats, cross_tab_feats) ≤ D_KL(p(cross_img_feats | cross_tab_feats) || p(cross_img_feats)p(cross_tab_feats))\n",
    "        Args:\n",
    "            cross_img_feats: 从图像模态计算的交互特征 (batch_size, feature_dim)\n",
    "            cross_tab_feats: 从表格模态计算的交互特征 (batch_size, feature_dim)\n",
    "        Returns:\n",
    "            loss: 互信息最小化的损失 (标量)\n",
    "        \"\"\"\n",
    "        # 计算 cross_img_feats 和 cross_tab_feats 的均值和对数方差\n",
    "        mu_img = cross_img_feats.mean(dim=0)\n",
    "        cross_img_feats = cross_img_feats.mean(dim=0)\n",
    "        log_var_img = torch.log(cross_img_feats.var(dim=0) + 1e-10)\n",
    "\n",
    "        mu_tab = cross_tab_feats.mean(dim=0)\n",
    "        cross_tab_feats = cross_tab_feats.mean(dim=0)\n",
    "        log_var_tab = torch.log(cross_tab_feats.var(dim=0) + 1e-10)\n",
    "\n",
    "        mu_img = mu_img.mean(dim=0)\n",
    "        mu_tab = mu_tab.mean(dim=0)\n",
    "\n",
    "        # 计算 KL 散度\n",
    "        var_img = torch.exp(log_var_img)\n",
    "        var_tab = torch.exp(log_var_tab)\n",
    "\n",
    "        kl_img = 0.5 * torch.sum(1 + log_var_img - mu_img ** 2 - var_img)\n",
    "        kl_tab = 0.5 * torch.sum(1 + log_var_tab - mu_tab ** 2 - var_tab)\n",
    "        # print(\"mu_img:\", mu_img)\n",
    "        # print(\"log_var_img:\", log_var_img)\n",
    "        # print(\"KL is \",(kl_img + kl_tab) / 2)\n",
    "        # 最小化 KL 散度\n",
    "        return -(kl_img + kl_tab) / 2\n",
    "\n",
    "    def forward(self, img, tab_con, tab_cat):\n",
    "        self.device = img.device\n",
    "\n",
    "        if self.con_fc_num == 0:\n",
    "            tab_con = None\n",
    "        if self.cat_fc_num == 0:\n",
    "            tab_cat = None\n",
    "\n",
    "        extracted_feats = self.backbone(img)\n",
    "        # (batch_size, hidden_dim, seq_len)\n",
    "\n",
    "        img_out = self.img_fc(extracted_feats)\n",
    "\n",
    "        table_features_embed, table_embed_out = self.tab_model(tab_con, tab_cat)\n",
    "\n",
    "\n",
    "        # cross_tab_feats = self.img_to_tab_cross_attn(extracted_feats, table_features_embed)\n",
    "        # cross_img_feats = self.tab_to_img_cross_attn(table_features_embed, extracted_feats)\n",
    "        # table_features_embed_permute = table_features_embed.permute(0, 2, 1)  # (batch_size, 8, in_dim)\n",
    "        table_features_embed_permute = table_features_embed.view(table_features_embed.size(0), -1)\n",
    "        table_features_embed_permute = table_features_embed_permute.unsqueeze(1)\n",
    "        table_features = table_features_embed_permute  # (batch_size, 8, target_dim)\n",
    "        query_image = self.image_cross_proj(extracted_feats.unsqueeze(1))  # (batch_size, 1, target_dim)\n",
    "        query_table = table_features\n",
    "\n",
    "        scores_img2tab = torch.matmul(query_image, query_table.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.cross_dim))  # (batch_size, 1, 1)\n",
    "        attn_weights_img2tab = F.softmax(scores_img2tab, dim=-1)\n",
    "        cross_tab_feats = torch.matmul(attn_weights_img2tab, query_table).squeeze(dim=1)  # (batch_size, target_dim)\n",
    "\n",
    "        scores_tab2img = torch.matmul(query_table, query_image.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.cross_dim))\n",
    "        attn_weights_tab2img = F.softmax(scores_tab2img, dim=-1)\n",
    "        cross_img_feats = torch.matmul(attn_weights_tab2img, query_image).squeeze(dim=1)\n",
    "\n",
    "        table_features_embed = table_features_embed.view(table_features_embed.size(0), -1)\n",
    "\n",
    "        concatenated_features = torch.cat(\n",
    "            [extracted_feats, cross_img_feats, table_features_embed,\n",
    "             cross_tab_feats],\n",
    "            dim=1\n",
    "        )\n",
    "        concat_out = self.concat_fc(concatenated_features)\n",
    "\n",
    "        return img_out, table_embed_out, concat_out, extracted_feats, table_features_embed, cross_img_feats, cross_tab_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81b7e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModelDVMWithTab(pl.LightningModule):\n",
    "    def __init__(self, model_name, n_num_features, cat_cardinalities, reverse=False):\n",
    "        super().__init__()\n",
    "        self.net_img_clf = ImageClassifier(model_name=model_name, n_num_features=n_num_features,\n",
    "                                           cat_cardinalities=cat_cardinalities)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=129)\n",
    "        self.reverse = reverse\n",
    "        self.valid_loader = self.val_dataloader()\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = DVMConCatImageDataset(\"dataset/DVM/dataset_valid.csv\", \"raw_dataset/DVM\")\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "\n",
    "        return valid_loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "\n",
    "        (table_features_con, table_features_cat, image_features), label = batch\n",
    "        img_out, table_embed_out, concat_out, extracted_feats, table_features_embed, cross_img_feats, cross_tab_feats = self.net_img_clf(image_features, table_features_con, table_features_cat)\n",
    "\n",
    "        img_loss = F.cross_entropy(img_out, label)\n",
    "        table_embed_loss = F.cross_entropy(table_embed_out, label)\n",
    "        concat_loss = F.cross_entropy(concat_out, label)\n",
    "        # loss = 1 * img_loss + 0.2 * table_embed_loss + 4 * concat_loss\n",
    "        loss = 1 * img_loss + 4 * concat_loss\n",
    "\n",
    "        cross_loss = self.net_img_clf.InfoMin_loss(extracted_feats, table_features_embed)\n",
    "        proj_loss = self.net_img_clf.InfoMax_loss(cross_img_feats, cross_tab_feats)\n",
    "\n",
    "        loss = loss - 0.2 * proj_loss + 40 * cross_loss\n",
    "\n",
    "        self.log(\"img_loss\", img_loss)\n",
    "        self.log(\"tab_embed_loss\", table_embed_loss)\n",
    "        self.log(\"concat_loss\", concat_loss)\n",
    "        self.log(\"proj_loss\", proj_loss)\n",
    "        self.log(\"cross_loss\", cross_loss)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        (table_features_con, table_features_cat, image_features), label = batch\n",
    "        img_out, table_embed_out, concat_out, extracted_feats, table_features_embed, cross_img_feats, cross_tab_feats = self.net_img_clf(image_features, table_features_con, table_features_cat)\n",
    "\n",
    "        img_loss = F.cross_entropy(img_out, label)\n",
    "        table_embed_loss = F.cross_entropy(table_embed_out, label)\n",
    "        concat_loss = F.cross_entropy(concat_out, label)\n",
    "        # loss = 1 * img_loss + 0.2 * table_embed_loss + 4 * concat_loss\n",
    "        loss = 1 * img_loss + 4 * concat_loss\n",
    "\n",
    "        cross_loss = self.net_img_clf.InfoMin_loss(extracted_feats, table_features_embed)\n",
    "        proj_loss = self.net_img_clf.InfoMax_loss(cross_img_feats, cross_tab_feats)\n",
    "\n",
    "        loss = loss - 0.2 * proj_loss + 40 * cross_loss\n",
    "\n",
    "        val_acc = self.test_acc(concat_out, label).item()\n",
    "        val_acc_img = self.test_acc(img_out, label).item()\n",
    "        val_acc_table = self.test_acc(table_embed_out, label).item()\n",
    "\n",
    "        self.log(\"val_img_loss\", img_loss)\n",
    "        self.log(\"val_tab_embed_loss\", table_embed_loss)\n",
    "        self.log(\"val_concat_loss\", concat_loss)\n",
    "        self.log(\"val_proj_loss\", proj_loss)\n",
    "        self.log(\"val_cross_loss\", cross_loss)\n",
    "        self.log(\"val_acc\", val_acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc_img\", val_acc_img, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc_tab\", val_acc_table, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (table_features_con, table_features_cat, image_features), label = batch\n",
    "        img_out, table_embed_out, concat_out, _, _, _, _ = self.net_img_clf(image_features, table_features_con, table_features_cat)\n",
    "        loss = F.cross_entropy(concat_out, label)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": concat_out.detach(),\n",
    "            \"img_preds\": img_out.detach(),\n",
    "            \"table_preds\": table_embed_out.detach(),\n",
    "            \"y\": label.detach()\n",
    "        }\n",
    "    def test_step_end(self, outputs):\n",
    "        test_acc = self.test_acc(outputs['preds'], outputs['y']).item()\n",
    "        self.log(\"test_acc\", test_acc, on_epoch=True, on_step=False)\n",
    "        img_test_acc = self.test_acc(outputs['img_preds'], outputs['y']).item()\n",
    "        self.log(\"test_acc_img\", img_test_acc, on_epoch=True, on_step=False)\n",
    "        table_test_acc = self.test_acc(outputs['table_preds'], outputs['y']).item()\n",
    "        self.log(\"test_acc_tab\", table_test_acc, on_epoch=True, on_step=False)\n",
    "        self.log(\"test_loss\", outputs[\"loss\"].mean(), on_epoch=True, on_step=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.net_img_clf.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56dc3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class DVMConCatImageDataset(Dataset):\n",
    "    def __init__(self, table_path: str, image_path: str):\n",
    "        self.table_path = table_path\n",
    "        self.image_path = image_path\n",
    "\n",
    "        image_size = 224\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "        self.table_df = pd.read_csv(self.table_path)\n",
    "        self.con_cols = ['Width', 'Length', 'Height', 'Wheelbase', 'Price', 'Adv_year', 'Reg_year', \n",
    "                 'Adv_month', 'Runned_Miles', 'Seat_num', 'Door_num', 'Engin_size', 'Entry_price']\n",
    "        self.cat_cols = ['Bodytype', 'Gearbox', 'Fuel_type']\n",
    "        for col in self.cat_cols:\n",
    "            if self.table_df[col].max() > 100:\n",
    "                self.table_df[col] = pd.cut(self.table_df[col], bins=10, labels=range(10))\n",
    "        for col in self.con_cols:\n",
    "            # if self.table_df[col].max() > 100:\n",
    "            #     self.table_df[col] = pd.cut(self.table_df[col], bins=100, labels=range(100))\n",
    "            # z-score normalization\n",
    "            col_data = torch.tensor(self.table_df[col].values, dtype=torch.float)\n",
    "            mean = torch.mean(col_data)\n",
    "            std = torch.std(col_data)\n",
    "            self.table_df[col] = torch.div(torch.sub(col_data, mean), std)\n",
    "        self.cat_cardinalities = self.table_df[self.cat_cols].max().tolist()\n",
    "        self.cat_cardinalities = [i+1 for i in self.cat_cardinalities]\n",
    "\n",
    "        self.data_indice = []\n",
    "\n",
    "        table_y = self.table_df[\"Genmodel_ID_encode\"]\n",
    "        table_X = self.table_df.drop(['Genmodel_ID_encode'], axis=1)\n",
    "\n",
    "        for index, row in tqdm.tqdm(table_X.iterrows(), total=len(table_X)):\n",
    "            label = table_y[index]\n",
    "            image_id = row[\"Image_path\"]\n",
    "\n",
    "            features = row.drop(\"Image_path\")\n",
    "            features_con = features[self.con_cols].to_numpy(dtype=float)\n",
    "            features_cat = features[self.cat_cols].to_numpy(dtype=int)\n",
    "            self.data_indice.append((image_id, features_con, features_cat, label))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id, features_con, features_cat, label = self.data_indice[index]\n",
    "        image_path = os.path.join(self.image_path, f\"{image_id}\")\n",
    "        image = self.transform(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        table_features_con = torch.tensor(features_con, dtype=torch.float)\n",
    "        table_features_cat = torch.tensor(features_cat, dtype=torch.long)\n",
    "        \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return (table_features_con, table_features_cat, image), label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DVMConCatImageDataset(\"dataset/DVM/dataset_train.csv\", \"raw_dataset/DVM\")\n",
    "valid_dataset = DVMConCatImageDataset(\"dataset/DVM/dataset_valid.csv\", \"raw_dataset/DVM\")\n",
    "test_dataset = DVMConCatImageDataset(\"dataset/DVM/dataset_test.csv\", \"raw_dataset/DVM\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, num_workers=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=8)\n",
    "\n",
    "n_num_features = len(train_dataset.con_cols)\n",
    "cat_cardinalities = train_dataset.cat_cardinalities\n",
    "\n",
    "model = ImageModelDVMWithTab(model_name='resnet', n_num_features=n_num_features, cat_cardinalities=cat_cardinalities)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=None, save_last=True, save_weights_only=False,\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=6\n",
    ")\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=9, mode='min')\n",
    "trainer = pl.Trainer(max_epochs=99, accelerator=\"gpu\", devices=device, callbacks=[checkpoint_callback, early_stopping], logger=logger)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "\n",
    "trainer.test(model=model, dataloaders=test_loader, ckpt_path='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
