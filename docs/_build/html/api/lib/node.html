

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NODE &mdash; LAMDA-TALENT  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            LAMDA-TALENT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html">How to Use TALENT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#cloning-the-repository">1. Cloning the Repository</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#running-experiments">2. Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#adding-new-methods">3. Adding New Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#configuring-hyperparameters">4. Configuring Hyperparameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#troubleshooting">5. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html">Methods in TALENT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#deep-learning-methods">Deep Learning Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#classical-methods">Classical Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#methodology-summary">Methodology Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependencies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html">Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#python-libraries">Python Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#optional-dependencies">Optional Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#additional-notes">Additional Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark_Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html">Benchmark Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#available-datasets">Available Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#downloading-datasets">Downloading Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#dataset-structure">Dataset Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#placing-datasets">Placing Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#using-datasets">Using Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#custom-datasets">Custom Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#task-types">Task Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Experimental_Results</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html">Experimental Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#results-summary">Results Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../core.html">Core Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning.html">Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../classical_methods.html">Classical Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib.html">Library Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Acknowledgements</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Acknowledgments</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">LAMDA-TALENT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><strong>NODE</strong></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api/lib/node.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="node">
<h1><strong>NODE</strong><a class="headerlink" href="#node" title="Link to this heading"></a></h1>
<p>A tree-mimic method that generalizes oblivious decision trees, combining gradient-based optimization with hierarchical representation learning.</p>
<section id="class-denseblock-nn-sequential">
<h2>class DenseBlock(nn.Sequential)<a class="headerlink" href="#class-denseblock-nn-sequential" title="Link to this heading"></a></h2>
<p>A dense block composed of multiple layers of tree-based modules (e.g., ODST) with residual connections. Each layer concatenates its output with the input features, creating a densely connected structure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">layer_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">tree_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">flatten_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">Module</span><span class="o">=</span><span class="n">ODST</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>input_dim</strong> <em>(int)</em> - Number of input features.</p></li>
<li><p><strong>layer_dim</strong> <em>(int)</em> - Number of trees (output features) per layer.</p></li>
<li><p><strong>num_layers</strong> <em>(int)</em> - Number of layers in the dense block.</p></li>
<li><p><strong>tree_dim</strong> <em>(int, optional, Default is 1)</em> - Number of outputs per tree.</p></li>
<li><p><strong>max_features</strong> <em>(Optional[int], Default is None)</em> - Maximum number of features to retain (truncates concatenated features if exceeded).</p></li>
<li><p><strong>input_dropout</strong> <em>(float, optional, Default is 0.0)</em> - Dropout rate applied to layer inputs during training.</p></li>
<li><p><strong>flatten_output</strong> <em>(bool, optional, Default is True)</em> - Whether to flatten the output or keep tree outputs separate.</p></li>
<li><p><strong>Module</strong> <em>(type, optional, Default is ODST)</em> - Module class to use for each layer (e.g., ODST).</p></li>
<li><p><strong>kwargs</strong> - Additional arguments passed to the module class.</p></li>
</ul>
<section id="forward-pass">
<h3><strong>Forward Pass</strong><a class="headerlink" href="#forward-pass" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(batch_size, …, input_dim)</cite>.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output tensor with shape:
- If <cite>flatten_output=True</cite>: <cite>(batch_size, …, num_layers * layer_dim * tree_dim)</cite>.
- If <cite>flatten_output=False</cite>: <cite>(batch_size, …, num_layers * layer_dim, tree_dim)</cite>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">to_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Converts integer tensor to one-hot encoding.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>y</strong> <em>(torch.Tensor)</em> - Input integer tensor of any shape.</p></li>
<li><p><strong>depth</strong> <em>(Optional[int], Default is None)</em> - Size of one-hot dimension. If None, inferred as <cite>max(y) + 1</cite>.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - One-hot encoded tensor with shape <cite>(*y.shape, depth)</cite>.</p></li>
</ul>
</section>
</section>
<section id="class-sparsemaxfunction-function">
<h2>class SparsemaxFunction(Function)<a class="headerlink" href="#class-sparsemaxfunction-function" title="Link to this heading"></a></h2>
<p>Applies the Sparsemax function, a sparse alternative to softmax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>input</strong> <em>(torch.Tensor)</em> - Input tensor of any shape.</p></li>
<li><p><strong>dim</strong> <em>(int, optional, Default is -1)</em> - Dimension along which to apply Sparsemax.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output tensor with same shape as input, values summing to 1 along <cite>dim</cite>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>grad_output</strong> <em>(torch.Tensor)</em> - Gradient of the loss with respect to the output.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Gradient of the loss with respect to the input.</p></li>
</ul>
</section>
<section id="class-entmax15function-function">
<h2>class Entmax15Function(Function)<a class="headerlink" href="#class-entmax15function-function" title="Link to this heading"></a></h2>
<p>Applies the Entmax 1.5 function, a sparse and smooth alternative to softmax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>input</strong> <em>(torch.Tensor)</em> - Input tensor of any shape.</p></li>
<li><p><strong>dim</strong> <em>(int, optional, Default is -1)</em> - Dimension along which to apply Entmax15.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output tensor with same shape as input, values summing to 1 along <cite>dim</cite>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>grad_output</strong> <em>(torch.Tensor)</em> - Gradient of the loss with respect to the output.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Gradient of the loss with respect to the input.</p></li>
</ul>
<section id="entmoid15-activation">
<h3><strong>Entmoid15 Activation</strong><a class="headerlink" href="#entmoid15-activation" title="Link to this heading"></a></h3>
</section>
</section>
<section id="class-entmoid15-function">
<h2>class Entmoid15(Function)<a class="headerlink" href="#class-entmoid15-function" title="Link to this heading"></a></h2>
<p>Efficient implementation of Entmax15 for binary classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>input</strong> <em>(torch.Tensor)</em> - Input tensor of any shape.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output tensor with values between 0 and 1.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>grad_output</strong> <em>(torch.Tensor)</em> - Gradient of the loss with respect to the output.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Gradient of the loss with respect to the input.</p></li>
</ul>
<section id="utility-modules">
<h3><strong>Utility Modules</strong><a class="headerlink" href="#utility-modules" title="Link to this heading"></a></h3>
</section>
</section>
<section id="class-lambda-nn-module">
<h2>class Lambda(nn.Module)<a class="headerlink" href="#class-lambda-nn-module" title="Link to this heading"></a></h2>
<p>Wraps a function into a PyTorch module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>func</strong> <em>(callable)</em> - Function to wrap.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>args</strong> - Positional arguments passed to the function.</p></li>
<li><p><strong>kwargs</strong> - Keyword arguments passed to the function.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>Any</strong> - Output of the function.</p></li>
</ul>
</section>
<section id="class-modulewithinit-nn-module">
<h2>class ModuleWithInit(nn.Module)<a class="headerlink" href="#class-modulewithinit-nn-module" title="Link to this heading"></a></h2>
<p>Base class for modules with data-aware initialization on the first batch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>args</strong> - Positional arguments for initialization.</p></li>
<li><p><strong>kwargs</strong> - Keyword arguments for initialization.</p></li>
</ul>
<p><strong>Note:</strong>
Must be implemented by subclasses to initialize module parameters using the first batch of data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>args</strong> - Positional arguments for the forward pass.</p></li>
<li><p><strong>kwargs</strong> - Keyword arguments for the forward pass.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output of the module’s forward pass.</p></li>
</ul>
</section>
<section id="class-odst-modulewithinit">
<h2>class ODST(ModuleWithInit)<a class="headerlink" href="#class-odst-modulewithinit" title="Link to this heading"></a></h2>
<p>A differentiable tree-based module that combines oblivious decision trees with sparse activation functions (sparsemax/sparsemoid) for end-to-end training. Designed as a drop-in replacement for <cite>nn.Linear</cite> with enhanced feature interaction capabilities.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">num_trees</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">tree_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">flatten_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">choice_function</span><span class="o">=</span><span class="n">sparsemax</span><span class="p">,</span> <span class="n">bin_function</span><span class="o">=</span><span class="n">sparsemoid</span><span class="p">,</span> <span class="n">initialize_response_</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">,</span> <span class="n">initialize_selection_logits_</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">,</span> <span class="n">threshold_init_beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">threshold_init_cutoff</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>in_features</strong> <em>(int)</em> - Number of input features.</p></li>
<li><p><strong>num_trees</strong> <em>(int)</em> - Number of decision trees in the module.</p></li>
<li><p><strong>depth</strong> <em>(int, optional, Default is 6)</em> - Depth of each tree (number of splits).</p></li>
<li><p><strong>tree_dim</strong> <em>(int, optional, Default is 1)</em> - Number of output channels per tree.</p></li>
<li><p><strong>flatten_output</strong> <em>(bool, optional, Default is True)</em> - If True, flattens output to <cite>(batch_size, num_trees * tree_dim)</cite>; otherwise, returns <cite>(batch_size, num_trees, tree_dim)</cite>.</p></li>
<li><p><strong>choice_function</strong> <em>(Callable, optional, Default is sparsemax)</em> - Function to compute feature weights (must map to a simplex, e.g., sparsemax).</p></li>
<li><p><strong>bin_function</strong> <em>(Callable, optional, Default is sparsemoid)</em> - Function to compute leaf weights (must map to [0, 1], e.g., sparsemoid).</p></li>
<li><p><strong>initialize_response_</strong> <em>(Callable, optional, Default is nn.init.normal_)</em> - Initializer for tree output responses.</p></li>
<li><p><strong>initialize_selection_logits_</strong> <em>(Callable, optional, Default is nn.init.uniform_)</em> - Initializer for feature selection logits.</p></li>
<li><p><strong>threshold_init_beta</strong> <em>(float, optional, Default is 1.0)</em> - Beta distribution parameter for data-aware threshold initialization (controls quantile selection).</p></li>
<li><p><strong>threshold_init_cutoff</strong> <em>(float, optional, Default is 1.0)</em> - Scaling factor for temperature initialization (controls sparsity of bin selections).</p></li>
</ul>
<section id="id1">
<h3><strong>Forward Pass</strong><a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>Processes input through oblivious decision trees with sparse activations.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>input</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(batch_size, …, in_features)</cite>.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>torch.Tensor</strong> - Output tensor with shape:
- If <cite>flatten_output=True</cite>: <cite>(batch_size, …, num_trees * tree_dim)</cite>.
- If <cite>flatten_output=False</cite>: <cite>(batch_size, …, num_trees, tree_dim)</cite>.</p></li>
</ul>
</section>
<section id="data-aware-initialization">
<h3><strong>Data-Aware Initialization</strong><a class="headerlink" href="#data-aware-initialization" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</pre></div>
</div>
<p>Initializes tree thresholds and temperatures based on input data (called automatically on first forward pass).</p>
<p><strong>Parameters:</strong>
* <strong>input</strong> <em>(torch.Tensor)</em> - Input data tensor with shape <cite>(batch_size, in_features)</cite> (used for initialization).
* <strong>eps</strong> <em>(float, optional, Default is 1e-6)</em> - Small epsilon to avoid division by zero.</p>
<p><strong>Initialization Logic:</strong>
1. <strong>Thresholds</strong>: Sampled from data quantiles using a Beta distribution (controlled by <cite>threshold_init_beta</cite>).
2. <strong>Temperatures</strong>: Scaled to ensure most data points fall in the linear region of <cite>bin_function</cite> (controlled by <cite>threshold_init_cutoff</cite>).</p>
</section>
<section id="repr">
<h3><strong>Repr</strong><a class="headerlink" href="#repr" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
<p>Returns a string representation of the ODST module with key parameters.</p>
<p><strong>References:</strong></p>
<p>Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data
Sergei Popov, Stanislav Morozov, Artem Babenko
arXiv preprint arXiv:1909.06312, 2019.</p>
<p>PDF <a class="reference external" href="https://arxiv.org/abs/1909.06312">https://arxiv.org/abs/1909.06312</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Read the Docs core team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>