

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ExcelFormer &mdash; LAMDA-TALENT  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            LAMDA-TALENT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html">How to Use TALENT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#cloning-the-repository">1. Cloning the Repository</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#running-experiments">2. Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#adding-new-methods">3. Adding New Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#configuring-hyperparameters">4. Configuring Hyperparameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#troubleshooting">5. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html">Methods in TALENT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#deep-learning-methods">Deep Learning Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#classical-methods">Classical Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html#methodology-summary">Methodology Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependencies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html">Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#python-libraries">Python Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#optional-dependencies">Optional Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dependencies.html#additional-notes">Additional Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark_Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html">Benchmark Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#available-datasets">Available Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#downloading-datasets">Downloading Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#dataset-structure">Dataset Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#placing-datasets">Placing Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#using-datasets">Using Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#custom-datasets">Custom Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#task-types">Task Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_datasets.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Experimental_Results</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html">Experimental Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#results-summary">Results Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental_results.html#conclusion">Conclusion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../core.html">Core Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning.html">Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../classical_methods.html">Classical Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib.html">Library Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Acknowledgements</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgements.html">Acknowledgments</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">LAMDA-TALENT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><strong>ExcelFormer</strong></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api/lib/excelformer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="excelformer">
<h1><strong>ExcelFormer</strong><a class="headerlink" href="#excelformer" title="Link to this heading"></a></h1>
<p>Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a “sure bet” solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs’ rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a “sure bet” solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning.</p>
<section id="feature-shuffling-augmentation">
<h2><strong>Feature Shuffling Augmentation</strong><a class="headerlink" href="#feature-shuffling-augmentation" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">batch_feat_shuffle</span><span class="p">(</span><span class="n">Xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</pre></div>
</div>
<p>Applies feature-wise shuffling between random pairs of samples in a batch.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>Xs</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(batch_size, features)</cite> or <cite>(batch_size, features, dim)</cite>.</p></li>
<li><p><strong>beta</strong> <em>(float, optional, Default is 0.5)</em> - Shape parameter for the Beta distribution controlling the shuffling rate.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>Xs_mixup</strong> <em>(torch.Tensor)</em> - Augmented tensor with randomly shuffled features.</p></li>
<li><p><strong>feat_masks</strong> <em>(torch.Tensor)</em> - Binary masks indicating which features were shuffled (shape <cite>(batch_size, features)</cite>).</p></li>
<li><p><strong>shuffled_sample_ids</strong> <em>(np.ndarray)</em> - Indices used for shuffling samples.</p></li>
</ul>
<p><strong>Description:</strong></p>
<p>Randomly shuffles features between pairs of samples in the batch based on a Beta distribution. Each feature in each sample has a probability of being replaced by the corresponding feature from another randomly selected sample. This creates new synthetic samples that combine features from different original samples.</p>
</section>
<section id="dimension-wise-shuffling-augmentation">
<h2><strong>Dimension-wise Shuffling Augmentation</strong><a class="headerlink" href="#dimension-wise-shuffling-augmentation" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">batch_dim_shuffle</span><span class="p">(</span><span class="n">Xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</pre></div>
</div>
<p>Applies dimension-wise shuffling between random pairs of samples in a batch.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>Xs</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(batch_size, features, dim)</cite>.</p></li>
<li><p><strong>beta</strong> <em>(float, optional, Default is 0.5)</em> - Shape parameter for the Beta distribution controlling the shuffling rate.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>Xs_mixup</strong> <em>(torch.Tensor)</em> - Augmented tensor with randomly shuffled dimensions.</p></li>
<li><p><strong>shuffle_rates</strong> <em>(torch.Tensor)</em> - Shuffling rates drawn from the Beta distribution (shape <cite>(batch_size,)</cite>).</p></li>
<li><p><strong>shuffled_sample_ids</strong> <em>(np.ndarray)</em> - Indices used for shuffling samples.</p></li>
</ul>
<p><strong>Description:</strong></p>
<p>Randomly shuffles entire dimensions between pairs of samples in the batch. For each sample and each dimension, a random decision is made whether to replace the entire dimension with the corresponding dimension from another sample. This creates synthetic samples that combine dimensions from different original samples.</p>
</section>
<section id="traditional-mixup-augmentation">
<h2><strong>Traditional Mixup Augmentation</strong><a class="headerlink" href="#traditional-mixup-augmentation" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mixup_data</span><span class="p">(</span><span class="n">Xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</pre></div>
</div>
<p>Applies traditional mixup augmentation by linearly interpolating between pairs of samples.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>Xs</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(batch_size, features)</cite>.</p></li>
<li><p><strong>beta</strong> <em>(float, optional, Default is 0.5)</em> - Shape parameter for the Beta distribution controlling the interpolation coefficient.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>mixed_X</strong> <em>(torch.Tensor)</em> - Augmented tensor created by mixing pairs of samples.</p></li>
<li><p><strong>lam</strong> <em>(float)</em> - Interpolation coefficient drawn from the Beta distribution.</p></li>
<li><p><strong>shuffle_sample_ids</strong> <em>(np.ndarray)</em> - Indices used for shuffling samples.</p></li>
</ul>
<p><strong>Description:</strong></p>
<p>Creates new synthetic samples by linearly interpolating between pairs of samples using a coefficient drawn from a Beta distribution. For each sample, another sample is randomly selected, and the new sample is computed as:mixed_sample = λ * sample1 + (1 - λ) * sample2
where λ is drawn from Beta(β, β). This method encourages the model to learn linear combinations of features, improving generalization.</p>
</section>
</section>
<section id="neural-network-utilities-and-components">
<h1><strong>Neural Network Utilities and Components</strong><a class="headerlink" href="#neural-network-utilities-and-components" title="Link to this heading"></a></h1>
<p>A collection of PyTorch modules, activation functions, optimization utilities, and helper functions for building and training neural networks.</p>
<section id="normalization-layers">
<h2><strong>Normalization Layers</strong><a class="headerlink" href="#normalization-layers" title="Link to this heading"></a></h2>
<section id="class-lambda-nn-module">
<h3>class Lambda(nn.Module)<a class="headerlink" href="#class-lambda-nn-module" title="Link to this heading"></a></h3>
<p>A simple wrapper module to apply a custom function as a PyTorch module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">ty</span><span class="o">.</span><span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p><strong>Parameters:</strong>
* <strong>f</strong> <em>(Callable)</em> - A function to apply in the forward pass.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Applies the wrapped function to the input.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> - Input tensor.</p>
<p><strong>Returns:</strong>
* Output of the function applied to <cite>x</cite>.</p>
</section>
<section id="class-rmsnorm-nn-module">
<h3>class RMSNorm(nn.Module)<a class="headerlink" href="#class-rmsnorm-nn-module" title="Link to this heading"></a></h3>
<p>Root Mean Square Layer Normalization, a variant of layer normalization that normalizes inputs using the root mean square.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong>
* <strong>d</strong> <em>(int)</em> - Model dimension (input feature size).
* <strong>p</strong> <em>(float, optional, Default is -1.0)</em> - Fraction of features to use for partial RMSNorm (range [0, 1]; disabled if &lt;0).
* <strong>eps</strong> <em>(float, optional, Default is 1e-5)</em> - Epsilon for numerical stability.
* <strong>bias</strong> <em>(bool, optional, Default is False)</em> - Whether to include a learnable bias term.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Applies RMS normalization to the input.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(…, d)</cite>.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Normalized tensor with the same shape as input.</p>
</section>
<section id="class-scalenorm-nn-module">
<h3>class ScaleNorm(nn.Module)<a class="headerlink" href="#class-scalenorm-nn-module" title="Link to this heading"></a></h3>
<p>Scale Normalization, a lightweight normalization that scales inputs by a learnable parameter divided by their norm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">clamp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p><strong>Parameters:</strong>
* <strong>d</strong> <em>(int)</em> - Model dimension (used to initialize the scale parameter as <cite>sqrt(d)</cite>).
* <strong>eps</strong> <em>(float, optional, Default is 1e-5)</em> - Epsilon added to norms for stability.
* <strong>clamp</strong> <em>(bool, optional, Default is False)</em> - Whether to clamp norms to a minimum of <cite>eps</cite> (instead of adding <cite>eps</cite>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Applies scale normalization to the input.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with shape <cite>(…, d)</cite>.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Normalized tensor with the same shape as input.</p>
</section>
</section>
<section id="activation-functions">
<h2><strong>Activation Functions</strong><a class="headerlink" href="#activation-functions" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reglu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>ReLUGLU activation: splits input into two halves, applies ReLU to the second half, and returns their product.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with even last dimension.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Output tensor with shape <cite>(…, d/2)</cite> where <cite>d</cite> is the input’s last dimension.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">geglu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>GELUGLU activation: splits input into two halves, applies GELU to the second half, and returns their product.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with even last dimension.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Output tensor with shape <cite>(…, d/2)</cite> where <cite>d</cite> is the input’s last dimension.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tanglu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>TanhGLU activation: splits input into two halves, applies Tanh to the second half, and returns their product.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor with even last dimension.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Output tensor with shape <cite>(…, d/2)</cite> where <cite>d</cite> is the input’s last dimension.</p>
<section id="class-reglu-nn-module">
<h3>class ReGLU(nn.Module)<a class="headerlink" href="#class-reglu-nn-module" title="Link to this heading"></a></h3>
<p>Module wrapper for <cite>reglu</cite> activation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Applies <cite>reglu</cite> activation.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Output of <cite>reglu(x)</cite>.</p>
</section>
<section id="class-geglu-nn-module">
<h3>class GEGLU(nn.Module)<a class="headerlink" href="#class-geglu-nn-module" title="Link to this heading"></a></h3>
<p>Module wrapper for <cite>geglu</cite> activation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Applies <cite>geglu</cite> activation.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input tensor.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Output of <cite>geglu(x)</cite>.</p>
</section>
</section>
<section id="optimization-utilities">
<h2><strong>Optimization Utilities</strong><a class="headerlink" href="#optimization-utilities" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">make_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">parameter_groups</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span>
</pre></div>
</div>
<p>Creates an optimizer instance from a string identifier.</p>
<p><strong>Parameters:</strong>
* <strong>optimizer</strong> <em>(str)</em> - Name of the optimizer (<cite>adabelief</cite>, <cite>adam</cite>, <cite>adamw</cite>, <cite>radam</cite>, <cite>sgd</cite>).
* <strong>parameter_groups</strong> - Parameters to optimize (typically from <cite>model.parameters()</cite>).
* <strong>lr</strong> <em>(float)</em> - Learning rate.
* <strong>weight_decay</strong> <em>(float)</em> - Weight decay (L2 penalty).</p>
<p><strong>Returns:</strong>
* <strong>optim.Optimizer</strong> - Initialized optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">make_lr_schedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">epoch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lr_schedule</span><span class="p">:</span> <span class="n">ty</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">ty</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ty</span><span class="o">.</span><span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">ty</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="n">ty</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">_LRScheduler</span><span class="p">],</span> <span class="n">ty</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ty</span><span class="o">.</span><span class="n">Any</span><span class="p">],</span> <span class="n">ty</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
</pre></div>
</div>
<p>Creates a learning rate scheduler.</p>
<p><strong>Parameters:</strong>
* <strong>optimizer</strong> <em>(optim.Optimizer)</em> - Optimizer to schedule.
* <strong>lr</strong> <em>(float)</em> - Base learning rate.
* <strong>epoch_size</strong> <em>(int)</em> - Number of steps per epoch.
* <strong>lr_schedule</strong> <em>(Optional[Dict])</em> - Scheduler configuration (defaults to <cite>{‘type’: ‘constant’}</cite>).</p>
<p><strong>Returns:</strong>
* <strong>Optional[optim.lr_scheduler._LRScheduler]</strong> - Learning rate scheduler.
* <strong>Dict</strong> - Scheduler configuration.
* <strong>Optional[int]</strong> - Number of warmup steps (if applicable).</p>
</section>
<section id="activation-function-helpers">
<h2><strong>Activation Function Helpers</strong><a class="headerlink" href="#activation-function-helpers" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_activation_fn</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ty</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>
</pre></div>
</div>
<p>Retrieves an activation function by name.</p>
<p><strong>Parameters:</strong>
* <strong>name</strong> <em>(str)</em> - Name of the activation (<cite>reglu</cite>, <cite>geglu</cite>, <cite>sigmoid</cite>, <cite>tanglu</cite>, or any function in <cite>torch.nn.functional</cite>).</p>
<p><strong>Returns:</strong>
* <strong>Callable</strong> - Activation function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_nonglu_activation_fn</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ty</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>
</pre></div>
</div>
<p>Retrieves the non-GLU counterpart of an activation (e.g., ReLU for ReGLU).</p>
<p><strong>Parameters:</strong>
* <strong>name</strong> <em>(str)</em> - Name of the GLU activation (<cite>reglu</cite>, <cite>geglu</cite>, or any function in <cite>torch.nn.functional</cite>).</p>
<p><strong>Returns:</strong>
* <strong>Callable</strong> - Non-GLU activation function.</p>
</section>
<section id="training-utilities">
<h2><strong>Training Utilities</strong><a class="headerlink" href="#training-utilities" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">load_swa_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">swa_model</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">AveragedModel</span><span class="p">)</span>
</pre></div>
</div>
<p>Loads a Stochastic Weight Averaging (SWA) state dict into a model.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>model</strong> <em>(nn.Module)</em> - Model to load weights into.</p></li>
<li><p><strong>swa_model</strong> <em>(optim.swa_utils.AveragedModel)</em> - SWA model with averaged weights.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_epoch_parameters</span><span class="p">(</span><span class="n">train_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">ty</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ty</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
</pre></div>
</div>
<p>Determines batch size and steps per epoch based on training data size.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>train_size</strong> <em>(int)</em> - Number of training samples.</p></li>
<li><p><strong>batch_size</strong> <em>(int or str)</em> - Batch size (or preset name: <cite>v1</cite>, <cite>v2</cite>, <cite>v3</cite>).</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>int</strong> - Batch size.</p></li>
<li><p><strong>int</strong> - Steps per epoch.</p></li>
</ul>
</section>
<section id="learning-rate-schedulers">
<h2><strong>Learning Rate Schedulers</strong><a class="headerlink" href="#learning-rate-schedulers" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_linear_warmup_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</pre></div>
</div>
<p>Computes learning rate for linear warmup.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>lr</strong> <em>(float)</em> - Base learning rate.</p></li>
<li><p><strong>n_warmup_steps</strong> <em>(int)</em> - Number of warmup steps.</p></li>
<li><p><strong>step</strong> <em>(int)</em> - Current step (1-based).</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>float</strong> - Warmup learning rate.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_manual_lr</span><span class="p">(</span><span class="n">schedule</span><span class="p">:</span> <span class="n">ty</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</pre></div>
</div>
<p>Retrieves a manually specified learning rate for an epoch.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>schedule</strong> <em>(List[float])</em> - List of learning rates per epoch.</p></li>
<li><p><strong>epoch</strong> <em>(int)</em> - Current epoch (1-based).</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><strong>float</strong> - Learning rate for the epoch.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_transformer_lr</span><span class="p">(</span><span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</pre></div>
</div>
<p>Computes learning rate using the Transformer schedule (Vaswani et al.).</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>scale</strong> <em>(float)</em> - Scale factor.</p></li>
<li><p><strong>d</strong> <em>(int)</em> - Model dimension.</p></li>
<li><p><strong>n_warmup_steps</strong> <em>(int)</em> - Number of warmup steps.</p></li>
<li><p><strong>step</strong> <em>(int)</em> - Current step.</p></li>
</ul>
<p><strong>Returns:</strong>
* <strong>float</strong> - Transformer learning rate.</p>
</section>
<section id="training-loop-helpers">
<h2><strong>Training Loop Helpers</strong><a class="headerlink" href="#training-loop-helpers" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">star</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ty</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ty</span><span class="o">.</span><span class="n">Any</span><span class="p">]</span>
</pre></div>
</div>
<p>Performs a single training step.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><strong>model</strong> <em>(nn.Module)</em> - Model to train.</p></li>
<li><p><strong>optimizer</strong> <em>(optim.Optimizer)</em> - Optimizer.</p></li>
<li><p><strong>loss_fn</strong> - Loss function.</p></li>
<li><p><strong>step</strong> - Function to compute model output from a batch.</p></li>
<li><p><strong>batch</strong> - Input batch.</p></li>
<li><p><strong>star</strong> <em>(bool)</em> - Whether the loss function takes multiple arguments (from <cite>step</cite> output).</p></li>
</ul>
<p><strong>Returns:</strong>
* <strong>Tensor</strong> - Loss value.
* <strong>Any</strong> - Model output.</p>
</section>
<section id="model-utilities">
<h2><strong>Model Utilities</strong><a class="headerlink" href="#model-utilities" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</pre></div>
</div>
<p>Asserts and casts input to a PyTorch tensor.</p>
<p><strong>Parameters:</strong>
* <strong>x</strong> <em>(torch.Tensor)</em> - Input to cast.</p>
<p><strong>Returns:</strong>
* <strong>torch.Tensor</strong> - Input as a tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_n_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
</pre></div>
</div>
<p>Counts the number of trainable parameters in a model.</p>
<p><strong>Parameters:</strong>
* <strong>m</strong> <em>(nn.Module)</em> - Model to inspect.</p>
<p><strong>Returns:</strong>
* <strong>int</strong> - Number of trainable parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_mlp_n_parameters</span><span class="p">(</span><span class="n">units</span><span class="p">:</span> <span class="n">ty</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span>
</pre></div>
</div>
<p>Counts parameters in an MLP with given layer sizes.</p>
<p><strong>Parameters:</strong>
* <strong>units</strong> <em>(List[int])</em> - List of MLP layer sizes (input to output).</p>
<p><strong>Returns:</strong>
* <strong>int</strong> - Total number of parameters.</p>
</section>
<section id="optimizer-helpers">
<h2><strong>Optimizer Helpers</strong><a class="headerlink" href="#optimizer-helpers" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</pre></div>
</div>
<p>Gets the current learning rate from an optimizer.</p>
<p><strong>Parameters:</strong>
* <strong>optimizer</strong> <em>(optim.Optimizer)</em> - Optimizer.</p>
<p><strong>Returns:</strong>
* <strong>float</strong> - Current learning rate.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Sets the learning rate for all parameter groups in an optimizer.</p>
<p><strong>Parameters:</strong>
* <strong>optimizer</strong> <em>(optim.Optimizer)</em> - Optimizer.
* <strong>lr</strong> <em>(float)</em> - New learning rate.</p>
</section>
<section id="device-utilities">
<h2><strong>Device Utilities</strong><a class="headerlink" href="#device-utilities" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
</pre></div>
</div>
<p>Gets the default device (CUDA if available, else CPU).</p>
<p><strong>Returns:</strong>
* <strong>torch.device</strong> - Default device.</p>
</section>
<section id="gradient-utilities">
<h2><strong>Gradient Utilities</strong><a class="headerlink" href="#gradient-utilities" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">get_gradient_norm_ratios</span><span class="p">(</span><span class="n">m</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
</pre></div>
</div>
<p>Computes the ratio of gradient norms to parameter norms for all parameters.</p>
<p><strong>Parameters:</strong>
* <strong>m</strong> <em>(nn.Module)</em> - Model to inspect.</p>
<p><strong>Returns:</strong>
* <strong>Dict</strong> - Mapping from parameter names to gradient/parameter norm ratios.</p>
</section>
<section id="error-handling">
<h2><strong>Error Handling</strong><a class="headerlink" href="#error-handling" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">is_oom_exception</span><span class="p">(</span><span class="n">err</span><span class="p">:</span> <span class="ne">RuntimeError</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>
</pre></div>
</div>
<p>Checks if a runtime error is due to out-of-memory (OOM).</p>
<p><strong>Parameters:</strong>
* <strong>err</strong> <em>(RuntimeError)</em> - Error to check.</p>
<p><strong>Returns:</strong>
* <strong>bool</strong> - True if the error is OOM-related.</p>
</section>
<section id="custom-optimizers">
<h2><strong>Custom Optimizers</strong><a class="headerlink" href="#custom-optimizers" title="Link to this heading"></a></h2>
<section id="class-radam-optim-optimizer">
<h3>class RAdam(optim.Optimizer)<a class="headerlink" href="#class-radam-optim-optimizer" title="Link to this heading"></a></h3>
<p>Rectified Adam optimizer, a variant of Adam with improved convergence properties.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">degenerated_to_sgd</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters:</strong>
* <strong>params</strong> - Parameters to optimize.
* <strong>lr</strong> <em>(float, optional, Default is 1e-3)</em> - Learning rate.
* <strong>betas</strong> <em>(Tuple[float, float], optional, Default is (0.9, 0.999))</em> - Momentum parameters.
* <strong>eps</strong> <em>(float, optional, Default is 1e-8)</em> - Epsilon for stability.
* <strong>weight_decay</strong> <em>(float, optional, Default is 0)</em> - Weight decay.
* <strong>degenerated_to_sgd</strong> <em>(bool, optional, Default is True)</em> - Whether to fall back to SGD for unstable cases.</p>
</section>
<section id="class-adabelief-optim-optimizer">
<h3>class AdaBelief(optim.Optimizer)<a class="headerlink" href="#class-adabelief-optim-optimizer" title="Link to this heading"></a></h3>
<p>AdaBelief optimizer, which adapts stepsizes based on “belief” in observed gradients.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-16</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fixed_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rectify</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">degenerated_to_sgd</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">print_change_log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>、
<strong>Parameters:</strong>
* <strong>params</strong> - Parameters to optimize.
* <strong>lr</strong> <em>(float, optional, Default is 1e-3)</em> - Learning rate.
* <strong>betas</strong> <em>(Tuple[float, float], optional, Default is (0.9, 0.999))</em> - Momentum parameters.
* <strong>eps</strong> <em>(float, optional, Default is 1e-16)</em> - Epsilon for stability.
* <strong>weight_decay</strong> <em>(float, optional, Default is 0)</em> - Weight decay.
* <strong>amsgrad</strong> <em>(bool, optional, Default is False)</em> - Whether to use AMSGrad variant.
* <strong>weight_decouple</strong> <em>(bool, optional, Default is True)</em> - Whether to use decoupled weight decay.
* <strong>fixed_decay</strong> <em>(bool, optional, Default is False)</em> - Whether weight decay is fixed (not scaled by lr).
* <strong>rectify</strong> <em>(bool, optional, Default is True)</em> - Whether to use rectified updates (like RAdam).
* <strong>degenerated_to_sgd</strong> <em>(bool, optional, Default is True)</em> - Whether to fall back to SGD for unstable cases.
* <strong>print_change_log</strong> <em>(bool, optional, Default is True)</em> - Whether to print configuration changes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
<p>Resets the optimizer state (exponential moving averages and step count).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Performs a single optimization step for AdaBelief.</p>
<p><strong>Parameters:</strong>
* <strong>closure</strong> <em>(callable, optional)</em> - A closure that reevaluates the model and returns the loss.</p>
<p><strong>Returns:</strong>
* <strong>float or None</strong> - Loss value if closure is provided, else None.</p>
<p><strong>Description:</strong>
Implements the AdaBelief optimization algorithm, which adapts step sizes based on the “belief” in observed gradients (measured by the variance of gradient residuals). Supports features like decoupled weight decay, rectified updates (similar to RAdam), and AMSGrad for stable convergence.</p>
</section>
</section>
<section id="additional-notes">
<h2><strong>Additional Notes</strong><a class="headerlink" href="#additional-notes" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>RAdam</strong>: Addresses the convergence issues of Adam in early training stages by rectifying the adaptive learning rate using the variance of gradient moments.</p></li>
<li><p><strong>AdaBelief</strong>: Extends Adam by incorporating gradient uncertainty (variance of residuals) into step size calculation, improving generalization in tasks like computer vision and NLP.</p></li>
<li><p>Both optimizers include fallbacks to SGD for unstable scenarios, ensuring robustness across different training regimes.</p></li>
</ul>
<p><strong>Referencses:</strong></p>
<p>ExcelFormer: A Neural Network Surpassing GBDTs on Tabular Data
Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Ziyi Chen, Jian Wu, Jimeng Sun
arXiv preprint arXiv:2301.02819, 2024.</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/2301.02819">https://arxiv.org/abs/2301.02819</a></p>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Read the Docs core team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>